# -*- coding: utf-8 -*-
"""DF_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4BjJgRhMbOX5EzvCTWEEDwTWfAYiZVX

# Install Modules
"""

"""# CODE

"""

import os
import re
import warnings
from collections import defaultdict
import PyPDF2
import pandas as pd
import spacy
from spacy.matcher import Matcher

# ========== SUPPRESS TRANSFORMER LOADING BARS ==========
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

# Suppress warnings
warnings.filterwarnings("ignore")

try:
    from sentence_transformers import SentenceTransformer, util
    EMBEDS_AVAILABLE = True
except Exception:
    EMBEDS_AVAILABLE = False

try:
    from transformers import pipeline, logging
    # Suppress transformer logging
    logging.set_verbosity_error()
    SENTIMENT_AVAILABLE = True
except Exception:
    SENTIMENT_AVAILABLE = False

# Configuration
#PDF_FOLDER = "/content/pdfs/"
MIN_SENTENCE_TOKENS = 6
AI_SIM_THRESHOLD = 0.40
EMBED_MODEL = "all-MiniLM-L6-v2"
SENTIMENT_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"

nlp = spacy.load("en_core_web_sm")

embedder = None
if EMBEDS_AVAILABLE:
    try:
        # Disable sentence-transformers progress bars
        embedder = SentenceTransformer(EMBED_MODEL, show_progress_bar=False)
    except Exception as e:
        warnings.warn(f"Could not load embedder ({EMBED_MODEL}): {e}")
        embedder = None
        EMBEDS_AVAILABLE = False

sentiment_analyzer = None
if SENTIMENT_AVAILABLE:
    try:
        # Disable all progress bars and verbose output
        sentiment_analyzer = pipeline(
            "sentiment-analysis", 
            model=SENTIMENT_MODEL,
            device=-1,  # Use CPU to avoid GPU memory issues
            token=None if 'HUGGINGFACE_HUB_TOKEN' not in os.environ else os.environ['HUGGINGFACE_HUB_TOKEN']
        )
    except Exception as e:
        warnings.warn(f"Could not load sentiment model ({SENTIMENT_MODEL}): {e}")
        sentiment_analyzer = None
        SENTIMENT_AVAILABLE = False

# If embeddings not available, prepare simple AI-term Doc vectors using spaCy as fallback
AI_TERMS = ["artificial intelligence", "machine learning", "automation", "robot", "algorithm", "ai"]
if not EMBEDS_AVAILABLE:
    AI_TERM_DOCS = [nlp(t) for t in AI_TERMS]
else:
    AI_TERM_EMBS = embedder.encode(AI_TERMS, convert_to_tensor=True, show_progress_bar=False)

def pdf_extract(pdf_path):
    text = ""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for p in reader.pages:
                page_text = p.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        warnings.warn(f"Error reading {pdf_path}: {e}")
    return text


def nlp_patterns():
    matcher = Matcher(nlp.vocab)

    productivity_pattern = [
        {"LIKE_NUM": True, "OP": "?"},
        {"TEXT": {"REGEX": "^%$"}, "OP": "?"},
        {"LEMMA": {"IN": ["increase", "decrease", "growth", "gain", "improve", "decline", "reduce"]}},
        {"LOWER": {"IN": ["in", "of", "for"]}, "OP": "?"},
        {"LOWER": {"IN": ["productivity", "efficiency", "output", "performance"]}}
    ]

    job_impact_pattern = [
        {"LEMMA": {"IN": ["create", "generate", "displace", "replace", "eliminate", "reduce"]}},
        {"LIKE_NUM": True, "OP": "?"},
        {"LOWER": {"IN": ["job", "jobs", "employment", "workers", "positions"]}, "OP": "?"}
    ]

    economic_impact_pattern = [
        {"TEXT": {"REGEX": "^\$|usd|£|€"}, "OP": "?"},
        {"LIKE_NUM": True, "OP": "?"},
        {"LOWER": {"IN": ["trillion", "billion", "million", "gdp", "economic", "economy", "impact", "contribution"]}}
    ]

    skill_wage_pattern = [
        {"LOWER": {"IN": ["wage", "salary", "income", "skill", "training", "reskill", "upskill"]}},
        {"LEMMA": {"IN": ["increase", "decrease", "require", "become", "improve", "decline"]}, "OP": "?"},
        {"LIKE_NUM": True, "OP": "?"},
        {"TEXT": {"REGEX": "^%$"}, "OP": "?"}
    ]

    timeframe_pattern = [
        {"LOWER": {"IN": ["by", "within", "in", "over", "from", "until"]}, "OP": "?"},
        {"LIKE_NUM": True, "OP": "?"},
        {"LOWER": {"IN": ["year", "years", "decade", "month", "months", "century"]}}
    ]

    matcher.add("PRODUCTIVITY_IMPACT", [productivity_pattern])
    matcher.add("JOB_IMPACT", [job_impact_pattern])
    matcher.add("ECONOMIC_IMPACT", [economic_impact_pattern])
    matcher.add("SKILL_WAGE_IMPACT", [skill_wage_pattern])
    matcher.add("TIMEFRAME", [timeframe_pattern])

    return matcher


# Semantic helpers

def sentence_is_informative(sent_text, doc_obj=None):
    text = sent_text.strip()
    if len(text.split()) < MIN_SENTENCE_TOKENS:
        return False

    # exclude common non-content markers
    meta_markers = ["©", "copyright", "author", "doi", "jel classification", "references", "fig.", "figure", "table", "acknowledg"]
    if any(m in text.lower() for m in meta_markers):
        return False

    if re.match(r"^[\W_]*$", text):
        return False

    return True


def ai_relevance_score(sent_text):
    if EMBEDS_AVAILABLE and embedder is not None:
        sent_emb = embedder.encode(sent_text, convert_to_tensor=True, show_progress_bar=False)
        sims = util.cos_sim(sent_emb, AI_TERM_EMBS)
        return float(sims.max())
    else:
        doc = nlp(sent_text)
        sims = [doc.similarity(t) for t in AI_TERM_DOCS]
        return max(sims)


def detect_domain_semantic(sent_text):
    domains = {
        "productivity": ["productivity", "efficiency", "output", "performance"],
        "employment": ["job", "employment", "worker", "labor", "occupation", "unemployment", "displacement"],
        "economic": ["economy", "gdp", "market", "revenue", "investment", "growth", "contribution"],
        "skills_wages": ["wage", "salary", "income", "skill", "training", "reskilling", "upskilling"],
        "healthcare": ["health", "hospital", "medical", "disease", "diagnosis"],
        "education": ["education", "school", "student", "teacher", "learning"],
        "policy": ["regulation", "policy", "law", "governance", "ethics", "privacy", "security"]
    }

    if EMBEDS_AVAILABLE and embedder is not None:
        sent_emb = embedder.encode(sent_text, convert_to_tensor=True, show_progress_bar=False)
        best = (None, -1.0)
        for k, kws in domains.items():
            dom_emb = embedder.encode(kws, convert_to_tensor=True, show_progress_bar=False)
            sim = float(util.cos_sim(sent_emb, dom_emb).max())
            if sim > best[1]:
                best = (k, sim)
        # apply a sensible threshold to avoid spurious matches
        return best[0] if best[1] > 0.35 else "general"
    else:
        # spaCy keyword fallback
        lowered = sent_text.lower()
        for k, kws in domains.items():
            if any(w in lowered for w in kws):
                return k
        return "general"


def extract_timeframes(doc):
    years = re.findall(r"\b(19|20)\d{2}\b", doc.text)
    tf = []
    if years:
        # return unique sorted years
        tf.extend(sorted(set(years), key=lambda x: years.index(x)))

    # look for explicit patterns like "within 5 years", "by 2030", "next decade"
    rel_patterns = [r"within\s+\d+\s+years?", r"in\s+the\s+next\s+decade", r"by\s+\d{4}", r"next\s+\d+\s+years?", r"last\s+\d+\s+years?", r"\d+\s+years?\s+ago"]
    for p in rel_patterns:
        m = re.findall(p, doc.text, flags=re.I)
        if m:
            tf.extend(m)

    # remove trivial single determiners and short tokens
    cleaned = [t for t in tf if len(t) > 2 and not t.lower() in ["the", "a"]]
    return cleaned


def analyze_sentence(sentence, doc_id, matcher):
    insights = []

    # Basic filtering
    if not sentence_is_informative(sentence):
        return insights

    doc = nlp(sentence)

    # semantic relevance to AI
    ai_score = ai_relevance_score(sentence)
    if ai_score < AI_SIM_THRESHOLD:
        return insights

    # Run matcher
    matches = matcher(doc)

    # collect impact verbs, magnitudes and entity targets via dependencies + NER
    impact_verbs = []
    magnitudes = []
    affected_entities = []

    negative_nouns = set(["inequality", "unemployment", "loss", "risk", "harm", "bias", "displacement"])

    for token in doc:
        if token.lemma_.lower() in ["increase", "decrease", "create", "displace", "automate", "improve", "reduce", "enhance", "eliminate", "transform", "decline", "boost", "hurt"]:
            impact_verbs.append({'verb': token.lemma_.lower(), 'text': token.text, 'position': token.i})
        if token.like_num or re.match(r"^\d+[\.,]?\d*$", token.text):
            magnitudes.append({'value': token.text, 'position': token.i})
        # NER-based entities
        if token.ent_type_ in ["ORG", "PRODUCT", "NORP", "PERSON", "GPE"]:
            affected_entities.append({'entity': token.lemma_.lower(), 'text': token.text, 'relationship': 'ner'})

    # dependency-based target extraction
    for verb in impact_verbs:
        vtok = doc[verb['position']]
        # subjects and objects give clues about what's affected
        for child in vtok.children:
            if child.dep_ in ["dobj", "pobj", "attr", "nsubj"]:
                affected_entities.append({'entity': child.lemma_.lower(), 'text': child.text, 'relationship': child.dep_})

    # Extract timeframes
    timeframes = extract_timeframes(doc)

    # Process matched patterns first (structured)
    for match_id, start, end in matches:
        span = doc[start:end]
        pattern_name = nlp.vocab.strings[match_id]

        primary_magnitude = magnitudes[0]['value'] if magnitudes else None

        # Semantic Domain Determination
        domain = None
        if pattern_name == "PRODUCTIVITY_IMPACT":
            domain = "productivity"
        elif pattern_name == "JOB_IMPACT":
            domain = "employment"
        elif pattern_name == "ECONOMIC_IMPACT":
            domain = "economic"
        elif pattern_name == "SKILL_WAGE_IMPACT":
            domain = "skills_wages"
        else:
            domain = detect_domain_semantic(sentence)

        # Context-aware sentiment
        impact_type = None
        sentiment_score = None
        if SENTIMENT_AVAILABLE and sentiment_analyzer is not None:
            try:
                sres = sentiment_analyzer(sentence)
                label = sres[0]['label']
                score = float(sres[0].get('score', 0.0))
                sentiment_score = score
                if label.lower() in ['positive', 'pos', 'label_2']:
                    impact_type = 'positive'
                elif label.lower() in ['negative', 'neg', 'label_0']:
                    impact_type = 'negative'
                else:
                    impact_type = 'neutral'
            except Exception:
                impact_type = 'unknown'
        else:
            # simple heuristic: if affected entities contain negative nouns, flip if verb looks positive
            heur = 'positive' if any(v['verb'] in ['increase', 'create', 'improve', 'boost'] for v in impact_verbs) else 'negative'
            if any(a['entity'] in negative_nouns for a in affected_entities):
                heur = 'negative'
            impact_type = heur

        insight = {
            'doc_id': doc_id,
            'sentence': sentence,
            'pattern_type': pattern_name,
            'matched_text': span.text,
            'ai_similarity': ai_score,
            'impact_verbs': [v['verb'] for v in impact_verbs],
            'affected_entities': [e['entity'] for e in affected_entities],
            'magnitudes': [m['value'] for m in magnitudes],
            'timeframes': timeframes,
            'sentence_length': len(doc),
            'domain': domain,
            'impact_type': impact_type,
            'primary_magnitude': primary_magnitude,
            'sentiment_score': sentiment_score
        }

        insights.append(insight)

    # Fall back to dependency analysis if no matcher patterns but there is meaningful structure
    if not insights and impact_verbs and affected_entities:
        domain = detect_domain_semantic(sentence)
        primary_magnitude = magnitudes[0]['value'] if magnitudes else None

        if SENTIMENT_AVAILABLE and sentiment_analyzer is not None:
            try:
                sres = sentiment_analyzer(sentence)
                label = sres[0]['label']
                sentiment_score = float(sres[0].get('score', 0.0))
                if label.lower() in ['positive', 'pos', 'label_2']:
                    impact_type = 'positive'
                elif label.lower() in ['negative', 'neg', 'label_0']:
                    impact_type = 'negative'
                else:
                    impact_type = 'neutral'
            except Exception:
                impact_type = 'unknown'
        else:
            impact_type = 'positive' if any(v['verb'] in ['increase', 'create', 'improve', 'boost'] for v in impact_verbs) else 'negative'
            if any(a['entity'] in negative_nouns for a in affected_entities):
                impact_type = 'negative'

        insight = {
            'doc_id': doc_id,
            'sentence': sentence,
            'pattern_type': 'DEPENDENCY_ANALYSIS',
            'matched_text': 'N/A',
            'ai_similarity': ai_score,
            'impact_verbs': [v['verb'] for v in impact_verbs],
            'affected_entities': [e['entity'] for e in affected_entities],
            'magnitudes': [m['value'] for m in magnitudes],
            'timeframes': timeframes,
            'sentence_length': len(doc),
            'domain': domain,
            'impact_type': impact_type,
            'primary_magnitude': primary_magnitude,
            'sentiment_score': sentiment_score
        }
        insights.append(insight)

    return insights


def extract_insights(text, doc_id):
    insights = []
    matcher = nlp_patterns()

    # split sentences
    doc = nlp(text)

    for sent in doc.sents:
        try:
            sentence_insights = analyze_sentence(sent.text, doc_id, matcher)
            insights.extend(sentence_insights)
        except Exception as e:
            warnings.warn(f"Error analyzing sentence: {e}")

    return insights


def create_nlp_enhanced_dataframes(all_insights):
    if not all_insights:
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    insights_df = pd.DataFrame(all_insights)

    for col in ['impact_verbs', 'affected_entities', 'magnitudes', 'timeframes']:
        if col in insights_df.columns:
            insights_df[col] = insights_df[col].apply(lambda x: x if isinstance(x, list) else [])

    # Helper function to safely convert to float
    def safe_float_conversion(value):
        try:
            # Attempt to clean and convert
            clean_value = str(value).strip()
            # Check if it looks like a standard number (integer or float)
            if re.fullmatch(r"^-?\d+(\.\d+)?$", clean_value):
                 return float(clean_value)
            # Check if it's a number with commas (e.g., 1,000,000)
            if re.fullmatch(r"^-?\d{1,3}(,\d{3})*(\.\d+)?$", clean_value):
                return float(clean_value.replace(',', ''))
            # Handle simple fractions (e.g., 1/2)
            if re.fullmatch(r"^\d+/\d+$", clean_value):
                numerator, denominator = clean_value.split('/')
                if int(denominator) != 0:
                    return float(numerator) / float(denominator)
            # Return None for values that can't be converted after cleaning
            return None
        except (ValueError, TypeError, ZeroDivisionError):
            return None  # Return None for values that can't be converted


    # DOMAIN ANALYSIS
    domain_data = []
    for domain in insights_df['domain'].fillna('general').unique():
        domain_insights = insights_df[insights_df['domain'] == domain]
        for impact_type in domain_insights['impact_type'].fillna('unknown').unique():
            type_insights = domain_insights[domain_insights['impact_type'] == impact_type]

            # Calculate average magnitude for this domain/type
            magnitudes = [safe_float_conversion(mag) for mag in type_insights['primary_magnitude'].dropna()]
            magnitudes = [m for m in magnitudes if m is not None]

            avg_magnitude = sum(magnitudes) / len(magnitudes) if magnitudes else 0

            common_verbs = []
            if not type_insights['impact_verbs'].explode().dropna().empty:
                common_verbs = ', '.join(type_insights['impact_verbs'].explode().value_counts().head(3).index.tolist())

            domain_data.append({
                'domain': domain,
                'impact_type': impact_type,
                'insight_count': len(type_insights),
                'avg_magnitude': avg_magnitude,
                'avg_ai_similarity': type_insights['ai_similarity'].mean(),
                'common_verbs': common_verbs
            })

    domain_df = pd.DataFrame(domain_data)

    # TEMPORAL ANALYSIS
    temporal_data = []
    all_timeframes = []

    for tf_list in insights_df['timeframes'].dropna():
        if isinstance(tf_list, list):
            all_timeframes.extend(tf_list)

    # garbage tokens
    def tf_clean(t):
        t = str(t).strip()
        t = re.sub(r"^[\W_]+|[\W_]+$", "", t)
        if len(t) <= 2:
            return None
        if t.lower() in ['the', 'a', 'an', 'term']:
            return None
        return t

    cleaned_tfs = [tf_clean(t) for t in all_timeframes]
    cleaned_tfs = [t for t in cleaned_tfs if t]

    tf_counts = pd.Series(cleaned_tfs).value_counts()

    for timeframe, count in tf_counts.head(20).items():
        timeframe_insights = insights_df[insights_df['timeframes'].apply(lambda x: timeframe in x if isinstance(x, list) else False)]
        if timeframe_insights.empty:
            continue
        # Calculate average magnitude for this timeframe, ignoring non-convertible values
        magnitudes = [safe_float_conversion(mag) for mag in timeframe_insights['primary_magnitude'].dropna()]
        magnitudes = [m for m in magnitudes if m is not None]

        temporal_data.append({
            'timeframe': timeframe,
            'insight_count': int(count),
            'dominant_domain': timeframe_insights['domain'].value_counts().index[0] if not timeframe_insights['domain'].value_counts().empty else 'unknown',
            'dominant_impact': timeframe_insights['impact_type'].value_counts().index[0] if not timeframe_insights['impact_type'].value_counts().empty else 'unknown',
            'avg_magnitude': sum(magnitudes) / len(magnitudes) if magnitudes else 0
        })

    temporal_df = pd.DataFrame(temporal_data)

    # PATTERN EFFECTIVENESS
    pattern_data = []
    for pattern_type in insights_df['pattern_type'].fillna('UNKNOWN').unique():
        p_ins = insights_df[insights_df['pattern_type'] == pattern_type]
        pattern_data.append({
            'pattern_type': pattern_type,
            'insight_count': len(p_ins),
            'avg_ai_similarity': p_ins['ai_similarity'].mean(),
            'success_rate': len(p_ins) / len(insights_df) * 100 if len(insights_df) > 0 else 0,
            'domains_found': ', '.join(p_ins['domain'].value_counts().head(3).index.tolist())
        })

    pattern_df = pd.DataFrame(pattern_data)

    # ENTITY ANALYSIS
    entity_data = []
    all_entities = []
    for ent_list in insights_df['affected_entities'].dropna():
        if isinstance(ent_list, list):
            all_entities.extend(ent_list)

    entity_counts = pd.Series(all_entities).value_counts().head(50)

    for entity, count in entity_counts.items():
        ent_ins = insights_df[insights_df['affected_entities'].apply(lambda x: entity in x if isinstance(x, list) else False)]
        if ent_ins.empty:
            continue

        # Calculate average magnitude for this entity
        magnitudes = [safe_float_conversion(mag) for mag in ent_ins['primary_magnitude'].dropna()]
        magnitudes = [m for m in magnitudes if m is not None] # Filter out None values

        entity_data.append({
            'affected_entity': entity,
            'mention_count': int(count),
            'impact_distribution': ent_ins['impact_type'].value_counts().to_dict(),
            'common_domains': ', '.join(ent_ins['domain'].value_counts().head(3).index.tolist()),
            'avg_magnitude_when_mentioned': sum(magnitudes) / len(magnitudes) if magnitudes else 0
        })

    entity_df = pd.DataFrame(entity_data)

    return insights_df, domain_df, temporal_df, pattern_df, entity_df

def process_pdfs(folder_path):
    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]
    if not pdf_files:
        return []

    all_insights = []

    for pdf_file in pdf_files:
        pdf_path = os.path.join(folder_path, pdf_file)
        doc_id = os.path.splitext(pdf_file)[0]
        text = pdf_extract(pdf_path)
        if not text.strip():
            continue
        insights = extract_insights(text, doc_id)
        all_insights.extend(insights)

    return all_insights

def main_nlp(folder_path=None):
    if folder_path is None:
        folder_path = "/content/pdfs"
    if not os.path.exists(folder_path):
        return None, None, None, None, None

    all_insights = process_pdfs(folder_path)
    if not all_insights:
        return None, None, None, None, None

    insights_df, domain_df, temporal_df, pattern_df, entity_df = create_nlp_enhanced_dataframes(all_insights)

    # Save to CSV
    insights_df.to_csv('ai_impact_claims.csv', index=False)
    domain_df.to_csv('domain_impact.csv', index=False)
    temporal_df.to_csv('timeframe_analysis.csv', index=False)
    pattern_df.to_csv('pattern_analysis.csv', index=False)
    entity_df.to_csv('entity_analysis.csv', index=False)

    return insights_df, domain_df, temporal_df, pattern_df, entity_df

if __name__ == '__main__':
    insights_df, domain_df, temporal_df, pattern_df, entity_df = main_nlp()